# -*- coding: utf-8 -*-
"""merge_user_film_pairs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17eQ3vVAfTIYLXaViDRmyiEnHPhV_xGNH
"""

from google.colab import drive
drive.mount('/content/drive')

"""## imports / set up methods"""

import json
from json import JSONDecodeError
from datetime import datetime as dt
import traceback
print(dt.now())

def read_f(path):
    try:
        f = open(path, "r")
        s = f.read()
        f.close()
        return s
    except FileNotFoundError:
        print("file not found")
        return None
    except Exception as e:
        raise e
#end

def append_f(path, s):
    f = open(path, "a+")
    f.write(s)
    f.close()
    return
#end

def write_f(path, s):
    f = open(path, "w+")
    f.write(s)
    f.close()
    return
#end

def load_json(path):
    s = read_f(path)
    if s is None: return {"failure": "path"}
    try:
        js = json.loads(s)
        return js
    except JSONDecodeError as jde:
        return {"failure" : jde, "traceback" : traceback.format_exc()}
#end


print(dt.now())

"""## global vars"""

folder_path = "/content/drive/MyDrive/4th_Year/ML Group Project/Sarah_data/"
user_file_path = folder_path + "country_groups_added/TYPE_users_X00_X99.json"
dst_path_all = folder_path + "writing/user_film_pairs.json"

print(dt.now())

all_films_path = folder_path.replace("Sarah_data", "Philip's Data") + "all_film_dataV3.json"
all_films_dict = load_json(all_films_path)
try:
    print(film_dict["failure"])
except:
    print("got valid film data")
print(dt.now())

groups_path = folder_path.replace("Sarah_data", "Shaun's Data") + "countries_by_continent.json"

"""
cg = load_json(groups_path)
try:
    print(country_groups["failure"])
except:
    cg["Antarctica"] = ["antarctica"]

    cg["Oceania"].append("new caledonia")
    #cg["Asia"].append("hong kong")
    #cg["Asia"].append("palestine")
    #cg["Asia"].append("taiwan")
    #cg["Europe"].append("kosovo")
    write_f(groups_path, (json.dumps(cg, indent=2)))
"""
country_groups = load_json(groups_path)
#print(country_groups)
try:
    print(country_groups["failure"])
except:
    print("got countries")

print(json.dumps(country_groups["Antarctica"]))
print(dt.now())
#"""

"""# Country Groups

#### set up vars/clear
"""

missed_path = folder_path + "country_groups_added/countries_missed.csv"
file_with_missing = folder_path + "country_groups_added/file_with_missing_groups.csv"

#clearing
def clear_csvs():
    write_f(missed_path, "")
    write_f(file_with_missing, "")
    print("cleared!!", dt.now())
#clear_csvs()
print(dt.now())

"""#### country_mapping"""

country_map = {
    "netherlands" : "kingdom of the netherlands",
    "the netherlands" : "kingdom of the netherlands",
    ".nl" : "kingdom of the netherlands",
    "nz" : "new zealand",
    "aotearoa" : "new zealand",
    "north macedonia" : "macedonia",
    "ireland" : "republic of ireland",
    "vatican" : "vatican city",
    "uk" : "united kingdom",
    "china" : "people's republic of china",
    "brasil" : "brazil",
    "puerto rico" : "united states",
    "méxico" : "mexico",
    "usa" : "united states",
    "us" : "united states",
    "us of a" : "united states",
    "america" : "united states",
    "america :(" : "united states",
    "the united states" : "united states",
    "dominican" : "dominican republic",
    "norge" : "norway",
    "probably norway" : "norway",
    "romania..." : "romania",
    "deutschland" : "germany",
    "perú" : "peru",
    "bosnia & herzegovina" : "bosnia and herzegovina",
    "antarctica" : None,
    "italo italians": "italy",
    "românia" : "romania",
    "mountain west, usa" : "united states",
    "italia" : "italy",
    "southwest norway": "norway",
    "a canadian" : "canada",
    "france." : "france",
    "czechia" : "czech republic",
    "türkiye" : "turkey",
    "españa" :"spain",
    "east coast" : "united states",
    "de" : "germany",
    "lost in the middle of south italy" : "italy",
    "kosovo" : "macedonia",
    "sverige" : "sweden",
    "belgie" : "belgium",
    "u.s.a." : "united states",
    "brasil." : "brazil",
    "deutschländ" : "germany",
    "deutschland, germany, tyskland" : "germany",
    "porto rico" : "united states",
    "united states of america" : "united states",
    "czechoslovakia" : "czech republic",
    "syrian arab republic" : "syria",
    "soviet union" : "russia",
    "palestinian territory" : "palestine",
    "congo" : "democratic republic of the congo",
    "yugoslavia" : "serbia",
    "east germany" : "germany",
    "west germany" : "germany",
    "lao people's democratic republic" : "laos",
    "libyan arab jamahiriya" : "libya"
}

"""#### find_group()"""

def find_group(c_str):
    c_str = c_str.lower()
    if c_str in country_map:
        c_str = country_map[c_str]
        #print(c_str)
    for group in country_groups:
        c_list = country_groups[group]
        if c_str in c_list:
            return (c_str, group)
    #end for
    return (c_str, None)
#end
print(dt.now())

"""#### get countries for all films"""

def get_film_countries():
    for film_lid in all_films_dict:
        film_info = all_films_dict[film_lid]
        for c in film_info["production_countries"]:
            (new_c, group) = find_group(c)
            if group is None:
                print("X:", new_c)
#end
print(dt.now())

"""# User-Film Merging

## get one user-film pair and return json

### get_user_film_pair
"""

def get_user_film_pair(user_json, film_json, dst_path):
    # numbers
    views = film_json["number_of_views"]
    rates = film_json["number_of_ratings"]
    likes = film_json["number_of_likes"]

    # production countries/continents (writes them in same format as user country for comparisons)
    prod_countries = []
    prod_continents = []
    for c in film_json["production_countries"]:
        (low_c, continent) = find_group(c)
        prod_countries.append(low_c)
        if continent is None:
            print("no continent for", low_c)
            if low_c is not None:
                append_f(missed_path, (low_c+"\n")) ## add to missing countries to be added to the map
                append_f(file_with_missing, (dst_path))
        else: prod_continents.append(continent)
    #end for
    prod_continents = (list(dict.fromkeys(prod_continents))) # remove duplicate continents

    # finances
    budg = film_json["budget"]
    if budg <= 0: budg = None
    revn = film_json["revenue"]
    if revn <= 0: budg = None
    prof = film_json["profit"]
    if budg is None or revn is None: ## ie if we're missing either budget/revenue info
        prof = None

    # user average ratings for director/genres/actors
    direc_avg = try_json(user_json, film_json, "director")
    director_avg = direc_avg[0] # straight average
    director_avg_minus = direc_avg[1] # average minus user total average
    genres = []
    genres_minus = []
    for i in range(len(film_json["genres"])):
        key = film_json["genres"][i]
        genre_avg = try_json(user_json, film_json, "genre", index=i)
        if genre_avg is not None:
            genres.append({key: genre_avg[0]}) # straight average
            genres_minus.append({key: genre_avg[1]}) # average minus user total average
    actors = []
    actors_minus = []
    for i in range(len(film_json["actor"])):
        key = "actor"+str(i)
        act_avg = try_json(user_json, film_json, "actor", index=i)
        if act_avg is not None:
            actors.append({key: act_avg[0]}) # straight avg
            actors_minus.append({key: act_avg[1]}) # average minus user total average

    # user country/continent == film prod country/continent
    if user_json["country"] is not None:
        user_prod_continent = False # check for continent first, only check for country if continent is True
        user_prod_country = False
        if user_json["country_group"] is not None:
            for c in prod_continents:
                if c == user_json["country_group"]:
                    user_prod_continent = True
                    break
            #end for
            if user_prod_continent:

                for c in prod_countries:
                    if c == user_json["country"]:
                        user_prod_country = True
                        break
                #end for
            #end if
        #end if
    else:
        user_prod_continent = None
        user_prod_country = None

    u_avg = user_json["average_rating"]

    try:
        language = film_json["original_langauge"]
    except KeyError:
        language = None

    try:
        month = (film_json["release_date"]).split("-")[1] # date format = "YYYY-MM-DD"
    except:
        month = None

    relevant = {
        # USER fields
        "user_lid" :             user_json["lid"],
        "user_rating_for_film" : user_json["ratings_by_film"][film_json["lid"]],
        "user_country" :         user_json["country"],
        "user_country_group" :   user_json["country_group"],
        "user_films_watched" :   len(user_json["ratings_by_film"]),
        "user_average_rating" :  user_json["average_rating"], ## since all of the other user-film averages are in relation to this, we might get an output which is +- the user's overall average rather than
        # FILM fields
        "film_lid" :             film_json["lid"],
        "film_avg_rating" :      film_json["avg_rating"],
        "film_total_views" :     views,
        "film_total_ratings" :   rates,
        "film_rate_ratio" :      (rates/views),
        "film_total_likes" :     likes,
        "film_like_ratio" :      (likes/views),
        "film_prod_companies":   film_json["production_companies"],
        "film_prod_counties" :   prod_countries,
        "film_prod_continents" : prod_continents,
        "film_release_year" :    film_json["release_year"],
        "film_release_month" :   month,
        "film_age" :             film_json["movie_age"],
        "film_runtime_min" :     film_json["runtime"],
        "film_franchise" :       film_json["in_franchise"],
        "film_budget" :          budg,
        "film_revenue" :         revn,
        "film_profit" :          prof,
        "film_language" :        language,
        # USER-FILM fields
        "user_from_prod_country" :      user_prod_country,
        "user_from_prod_continent" :    user_prod_continent,
        "user_director_avg" :           director_avg,
        "user_director_avg_minus avg" : director_avg_minus,
        "user_genres_avg" :             genres, ## genres are lists of dicts for 1-hot-encoding = {genre_name : avg}
        "user_genres_avg_minus_avg" :   genres_minus,
        "user_actors_avg" :             actors,
        "user_actors_avg_minus" :       actors_minus,
        "film_rating_minus_user_avg" :  (film_json["avg_rating"]-user_json["average_rating"]),
    }
    return relevant
#end


print(dt.now())

"""### try_json"""

d = "director"
a = "actor"
g = "genre"
u_pref = "average_rating_by_"

def try_json(user_json, film_json, field, index=0):
    if field == d:
        entry = film_json[d]
    elif field == a:
        entry = film_json[(a)][index]
    elif field == g:
        entry = film_json[(g+"s")][index]
    else: return None
    #print(entry, end=" : ")
    #end if
    try:
        u_x_avg = user_json[(u_pref+field)][entry]["avg"]
        u_x_avg_minus = user_json[(u_pref+field)][entry]["avg_minus_tot_avg"]
        #print("got")
        return (u_x_avg, u_x_avg_minus)
    except KeyError as ke:
        return (user_json["average_rating"], 0) # hasn't seen a film of this genre before: return average
    except Exception as e:
        print("Something happened: ", e)
        print(traceback.format_exc())
        return None
    #end try
#end

print(dt.now())

"""## get user-film pairs for 1 user (and write all to json)"""

def write_film_pairs_for_user(user_json, path=dst_path_all, first_user=False):
    first_elem = False
    if first_user: first_elem = True ## first line in the file needs to not be preceeded by a comma
    user_pairs_str = ""
    for film_lid in user_json["ratings_by_film"]:
        if film_lid in all_films_dict:
            film_js = all_films_dict[film_lid]
            uf_pair = get_user_film_pair(user_json, film_js, dst_path=path)
            add_s = "\n" + json.dumps(uf_pair)
            if first_elem: first_elem = False
            else: add_s = "," + add_s
            user_pairs_str = user_pairs_str + add_s
        #end if
    #end for
    if not (len(user_pairs_str) == 0):
        append_f(path, user_pairs_str)
    return

print(dt.now())

"""##### test"""

# user_file_path = folder_path + "country_groups_added/TYPE_users_X00_X99.json"
# dst_path = folder_path + "writing/user_film_pairs.json"

def test_film_pairs_for_pop_099():
    start = dt.now()
    print(start)
    b_path = dst_path.replace("user", "test_pop_099")
    all = load_json(user_file_path.replace("TYPE", "pop").replace("X", "0"))
    write_f(b_path, "[")
    first_user = True
    i = 0
    for u in all["users"]:
        print(i)
        write_film_pairs_for_user(u, path=b_path, first_user=first_user)
        if first_user: first_user= False
        i = i+1
    #end for
    append_f(b_path, "\n]")
    res = load_json(b_path)
    try:
        print(res["failure"])
        try: print(res["traceback"])
        except: pass
        return
    except:
        pass
    print("json valid!")
    end = dt.now()
    print("time:", (end-start))
    return
#end

print(dt.now())
#test_film_pairs_for_pop_099()

"""## get user-film pairs for file of 100 users"""

# user_file_path = folder_path + "country_groups_added/TYPE_users_X00_X99.json"
dst_path = folder_path + "writing/TYPE_X00_X99_film_pairs.json"

def user_film_pairs_for_file(index, ut="pop"):
    start = dt.now()
    print(ut, index)
    this_dstpath = dst_path.replace("TYPE", ut).replace("X", str(index))
    all = load_json(user_file_path.replace("TYPE", ut).replace("X", str(index)))
    write_f(this_dstpath, "[")
    first_user = True
    for u in all["users"]:
        try:
            write_film_pairs_for_user(u, path=this_dstpath, first_user=first_user)
        except Exception as e:
            # stops for any reason, write to invalid and return (I'll come back to these)
            print("stopped at user", all["users"].index(u), u["lid"], "because\n",e,"\nwritten to inv_json")
            print(traceback.format_exc())
            write_f(this_dstpath.replace("writing", "inv_json"), read_f(this_dstpath))
            return
        if first_user: first_user= False
    #end for
    append_f(this_dstpath, "\n]")
    print(this_dstpath)
    res_str = read_f(this_dstpath)
    try:
        res_js = json.loads(res_str)
        print("json valid!")
        write_f(this_dstpath.replace("writing", "valid_user_film_merge"), json.dumps(res_js))
    except JSONDecodeError:
        write_f(this_dstpath.replace("writing", "inv_json"), res_str)
        print("writing to inv_json folder")
        return
    except Exception as e:
        print("Something Happened", e)
        print(traceback.format_exc())


    end = dt.now()
    print("time:", (end-start))
    return
#end
print(dt.now())

"""## loops through all user files: loads f into a dict and gets all pairs

pop 25
stopped at user 10 because KeyError: 'country_group'
written to inv_json
"""

def some_pairs(max, min=0, ut="pop"):
    start = dt.now()
    print(start)
    for i in range(min, max+1):
            user_film_pairs_for_file(index=i, ut=ut)
    #end for
    print("done with", ut, min, "-", max, "in:", (dt.now()-start))
    #end
#end

#some_pairs(max=38, min=19)
some_pairs(max=25,min=25, ut="pop")

def all_pairs():
    start = dt.now()
    print(start)
    for ut in ["pop", "gen"]:
        ra = 39
        if ut == "gen": ra = 58
        for i in range(ra):
            user_film_pairs_for_file(index=i, ut=ut)
        #end for
        print("done with", ut)
    #end
    print("all done. time taken: ", (dt.now()-start))
#end

#all_pairs()

"""# Get User Country Groups

### by file
"""

def write_groups_by_file(all_json, dstpath):
    users_in_file = len(all_json["users"])
    not_added = users_in_file ## decrease by 1 when
    for i in range(users_in_file):
        u = all_json["users"][i]
        #print(i, end=": ")
        try:
            t = u["country_group"]
            not_added = not_added-1
            continue ## next loop
        except KeyError:
            pass
        if u["country"] is None:
            all_json["users"][i]["country_group"] = None
            not_added = not_added-1
            #print("country None")
        else:
            uc = (u["country"]).lower()
            (c, gr) = find_group(uc)
            if c is not None:
                all_json["users"][i]["country"] = c
                all_json["users"][i]["country_group"] = gr
                not_added = not_added-1
                #print("group added", uc, "->", gr)
            else:
                append_f(missed_path, (uc+"\n"))
        #end if
    #end for
    if not_added > 0:
        append_f(file_with_missing, (dstpath+"\n"))
        print("added to csv for", not_added, "missing users")
    else: print("all countries accounted for!")
    write_f(dstpath, json.dumps(all_json))
    print("    written")
    return
#end

def test():
    print(dt.now())
    p = folder_path+"user_files/gen_users_000_099.json"
    t = load_json(p)
    try:
        print("couldn't get json:", t["failure"])
        return
    except:
        pass
    add_groups_by_file(t, p.replace("user_files", "country_groups_added"))

#test()
print(dt.now())

"""### run ALL"""

check_path = folder_path + "user_files/TYPE_users_X00_X99.json"
def add_country_groups_all():
    # 0 - 57 gen
    # 0 - 38 pop
    for ut in ["pop", "gen"]:
        print(ut, dt.now())
        utpath = check_path.replace("TYPE", ut)
        ra = 39
        if ut == "gen": ra = 58

        for i in range(ra):
            ipath = utpath.replace("X", str(i))
            s = read_f(ipath)
            if s is None:
                print("can't find",ut,"users", i)
                continue
            else:
                try:
                    i_all = json.loads(s)
                    ipath = ipath.replace("user_files", "country_groups_added")
                    print(ut, i, end=": ")
                    write_groups_by_file(i_all, ipath)
                except JSONDecodeError as jde:
                    print("got error for pop", path, ":", jde)
                    print(traceback.format_exc())
        #end for
        print("done with", ut)
    #end for
add_country_groups_all()

"""### run from CSV"""

def add_country_groups_csv():
    l_str = read_f(folder_path + "country_groups_added/file_with_missing_groups.csv")
    l = l_str.split("\n")
    write_f((folder_path + "country_groups_added/file_with_missing_groups.csv"), "")
    for path in l:
        s = read_f(path)
        if s is None:
            print("can't find users", path)
            continue
        else:
            try:
                i_all = json.loads(s)
                print(path, ": ")
                write_groups_by_file(i_all, path)
            except JSONDecodeError as jde:
                print("got error for", path, ":", jde)
                print(traceback.format_exc())
        #end for
    #end for
    print("done")
#add_country_groups_csv()

"""# previous code:"""

inv_test_path = folder_path + "inv_json/TYPE_users_X00_X99_film_pairs.json"

def test_json_files_loop(indices, ut="pop"):
    for i in indices:
        str_i = read_f(inv_test_path.replace("TYPE", ut).replace("X", str(i)))
        try:
            js_i = json.loads(str_i)
            print(ut, i, "valid")
        except JSONDecodeError as jde:
            print(ut, i, "invalid\n        ", jde)
    return
#end

#test_json_files_loop([37,35,25,18,5], ut="pop")
#test_json_files_loop([30,29,26,8], ut="gen")

"""## function for all_films_V2.json"""

folder_path = "/content/drive/MyDrive/4th_Year/ML Group Project/Sarah_data/safety/"

## uses old unmerged film data from Philip's Data/all_filmsV2.json 10/Dec/2020 8pm

"""
{  "34722": {
    "name": "Inception",
    "url": "/film/inception/",
    "lid": "34722",
    "tmdb_id": "27205",
    "number_of_ratings": 536165,
    "avg_rating": 4.18,
    "genres": [
      "Science Fiction",
      "Action",
      "Adventure"
    ],
    "director_url": "/director/christopher-nolan/",
    "actors_urls": [
      "/actor/leonardo-dicaprio/",
      "/actor/ken-watanabe/",
      "/actor/joseph-gordon-levitt/",
      "/actor/marion-cotillard/",
      "/actor/elliot-page/"
    ],
    "number_of_likes": 353380,
    "number_of_views": 926820
  }}
"""

def get_relevant_fields(user_json, film_json):
    relevant = {}
    relevant["user_lid"] = user_json["lid"]
    relevant["film_lid"] = film_json["lid"]
    relevant["user_country"] = user_json["country"]
    relevant["user_films_watched"] = len(user_json["ratings_by_film"])
    relevant["film_total_watches"] = film_json["number_of_views"]
    relevant["film_total_likes"] = film_json["number_of_likes"]
    relevant["director"] = try_json(user_json, film_json, "director")
    for i in range(len(film_json["genres"])):
        key = film_json["genres"][i]
        relevant[key] = try_json(user_json, film_json, "genre", index=i)
    for i in range(5):
        key = "actor"+str(i)
        relevant[key] = try_json(user_json, film_json, "actor", index=i)
    print(json.dumps(relevant, indent=2))
    return relevant
#end

def try_json(user_json, film_json, field, index=0):
    if field == "director":
        x = film_json[(field+"_url")]
        x = x.split("/")[2]
    elif field == "actor":
        x = film_json[(field+"s_urls")][index]
        x = x.split("/")[2]
    elif field == "genre":
        #print(film_json)
        x = film_json[(field+"s")][index]
    else: return None
    print(x, end=" : ")
    #end if
    try:
        u_x_avg = user_json[("average_rating_by_"+field)][x]["avg_minus_tot_avg"]
        print("got x")
        return u_x_avg
    except KeyError as ke:
        print("no x", ke)
        return None
    except Exception as e:
        print("Something happened: ", e)
        print(traceback.format_exc())
    #end try
#end

def test_get_revelant_fields():
    inception_lid = "34722"
    film_data_path = folder_path.replace("Sarah_data/safety/", "Philip's Data/all_filmsV2.json")
    all_films = json.loads(read_f(film_data_path).lower())
    inception = all_films[inception_lid]
    #print(json.dumps(inception,indent=2), "\n")

    # user who has watched Inception: "captstevezissou" from 000_099.json
    captstevezissou = {}
    all = json.loads( read_f((folder_path+"gen_users_5100_5199.json")) )
    for u in all["users"]:
        if u["lid"] == "captstevezissou":
            captstevezissou = u
            break
    #end for
    #print(json.dumps(captstevezissou, indent=2), "\n")
    get_relevant_fields(captstevezissou, inception)
    return

print(dt.now())
#test_get_revelant_fields()

"""## I could have sworn someone had "antarctica" as their location but I guess not"""

src_path = folder_path+"user_files/TYPE_users_X00_X99.json"
def find_artarc():
    for ut in ["gen", "pop"]:
        for i in range(59):
            ipath = src_path.replace("TYPE", ut).replace("X", str(i))
            s = read_f(ipath)
            if s is None:
                continue
            antarc_count = s.count("antarctica")
            antarc_count = antarc_count + s.count("antartica")
            if antarc_count > 0:
                print(ipath)
                return
            else: print("not", ut, i)
        #end for
    #end for
#end
#find_artarc()
