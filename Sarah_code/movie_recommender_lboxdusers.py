# -*- coding: utf-8 -*-
"""5_Movie_Recommender_LBoxdUsers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_UngQqiXn8f_lLZRcEXvZfhPjE8pnvuw
"""

from google.colab import drive
drive.mount('/content/drive')

"""# IMPORTS"""

"""
# !pip install letterboxd
import letterboxd
from letterboxd.services.auth import Authentication
from letterboxd import letterboxd as lboxd
from letterboxd.api import API

# !pip install tmdbv3api
import tmdbv3api as tmdb
"""

from bs4 import BeautifulSoup as BSoup
import requests
from datetime import datetime as dt
import random 
import traceback
import copy
import json
import csv 
from json import JSONDecodeError

print(dt.now())

"""# Set Up Global Vars"""

def read_f(path):
    try:
        f = open(path, "r")
        s = f.read()
        f.close()
        return s
    except FileNotFoundError:
        print("file not found")
        return None
    except Exception as e:
        raise e
#end

def append_f(path, s):
    f = open(path, "a+")
    f.write(s)
    f.close()
    return
#end

def write_f(path, s):
    f = open(path, "w+")
    f.write(s)
    f.close()
    return
#end
print(dt.now())

folder_path = "/content/drive/MyDrive/4th_Year/ML Group Project/Sarah_data/"

api_file_path = folder_path.replace("Sarah_data/","API_keys/google_maps_api.txt")
maps_api_key = read_f(api_file_path)
print(maps_api_key)

pop_usernames_csv_path = (folder_path + "pop_usernames.csv")
pop_users_json_path = (folder_path + "pop_users.json")
test_json_path = (folder_path + "test_users.json")
film_info_path = (folder_path + "sarah_film_info.json") 

user_films_csv_path = (folder_path + "user_films.csv")


film_info_dict = {} # {LID : {"director":[], "actors":[],"genres":[]}}
fi_str = read_f(film_info_path)
if fi_str is None:
    print("file not found")
else:
    if fi_str == "":
        print("no films found")
    else:
        try:
            film_info_dict = json.loads(fi_str)
            print("films found =", len(film_info_dict))
        except:
            print("whoops")


print("yes", fi_str.count("elliot-page"))
print("no", fi_str.count("ellen-page"))

d_sp = ""
for i in range(26):
    d_sp = d_sp + " "
#end

print(dt.now())

"""# Maps API"""

def get_country(location_str, api_key, printbool=False, printjson=False):
    # country name = json["results"][i]["address_components"][j]["long_name"] (if "country" in ["types"])
    # for all results, for all address_components, if type==country, get long_name
    if location_str is None:
        if printbool: print("location_str is None")
        return None

    address = str.replace(location_str, " ", "+")
    maps_base_url = ("https://maps.googleapis.com/maps/api/geocode/json")
    maps_params = {"address": address, "key": maps_api_key}

    # JSON labels
    ctry = "country"
    adr_comps = "address_components"
    poi = "point_of_interest"
    ln = "long_name"
    try: 
        page = requests.get(maps_base_url, params=maps_params, allow_redirects=True)
        json_str = page.text
        if printjson: print(json_str)
        loc_json = json.loads(json_str)
        if loc_json["status"] == "OK":
            results = loc_json["results"]
            
            for r in results:
                if ctry in r["types"]:
                    if printbool: print(location_str, " is a ", ctry)
                    return location_str
                elif poi in r["types"]:
                    if printbool: print(location_str, " is a ", poi, " (",ctry," = None)")
                    return None
                #end if
                comps = r["address_components"]
                for c in comps:
                    if ctry in c["types"]:
                        country = c[ln]
                        if printbool: print(location_str, " -> ", ctry," = ", country, "\n")
                        return country
                    #end if
                #end for
            #end for
        else:
            if printbool: print(location_str, " returned ", loc_json["status"], " (",ctry," = None)")
            return None
        #end if
        return None
    except Exception as e:
        if printbool: print(traceback.format_exc())
        if printbool: print("couldn't get country of ", location_str, " (",ctry," = None)\n", e)
        return None 
#end get_country()

print(dt.now())

"""# LBoxd_User class"""

lboxd_url = "https://letterboxd.com"
director = "director"
actor = "actor"
genre = "genre"
url = "url"
name = "name"

class LBoxd_User():
    def __init__(self, maps_api_key, profile_url=None, username=None, time_log=False):
        self.lid = None
        self.profile_url = None
        if profile_url is not None:
            if profile_url[-1] == "/": self.profile_url = profile_url
            else: self.profile_url = (profile_url + "/")
            
            self._generate_lid()
            if username is not None and username != self.lid: raise ValueError("URL does not match username provided")

        elif profile_url is None and username is not None:
            self.lid = username
            self.profile_url = (lboxd_url + "/" + self.lid + "/")
        else: 
            error_s = ("please initialise LBoxd_User object with one of profile_url and username\n" + "Found: profile_url=" + profile_url + " and username=" + username)
            raise ValueError(error_s)
        #end if

        self.time_log = time_log
        setup_time_start = dt.now()
        s = (str(setup_time_start) + " : " + self.lid + " START")
        print(s)
        #end if
        self.films_added_to_dict = 0

        self.maps_api_key = maps_api_key

        profile_page = requests.get(self.profile_url, allow_redirects=True)
        self.profile_bsoup = BSoup(profile_page.content, 'html.parser')
        self._generate_name()

        self.location_str = None
        self._generate_location_str()
        self.country = get_country(self.location_str, self.maps_api_key)

        self.ratings_base_url = (self.profile_url + "films/ratings/")
        self.ratings_pages_bsoups = {}
        self._generate_ratings_pages()
        self._generate_ratings_pages_bsoups()

        self.ratings = {}
        self._generate_ratings()
        if len(self.ratings) is 0:
            end_time = dt.now()
            s = (str(end_time) + " : " + str(end_time - setup_time_start) + " : " + self.lid + " END: no films")
            print(s)
            return

        self._setup_averages()
        self.json = None
        end_time = dt.now()
        s = (str(end_time) + " : " + str(end_time - setup_time_start) + " : " + self.lid + " END")
        print(s)
    #end init()

    def _setup_averages(self):
        self._generate_average_rating()
        self.average_rating_by_genre = self._generate_average_rating_by_X(genre)
        self.average_rating_by_director = self._generate_average_rating_by_X(director)
        self.average_rating_by_actor = self._generate_average_rating_by_X(actor)
    #end
    
    def _generate_name(self):
        name_div = self.profile_bsoup.find("div", {"class" : "profile-name-wrap"})
        try:
            self.name = (name_div.find("h1")).text
        except: self.name = self.lid
        return
    #end _generate_name()

    def _generate_location_str(self):
        location_pin_d = "M4.25 2.735a.749.749 0 111.5 0 .749.749 0 11-1.5 0zM8 4.75c0-2.21-1.79-4-4-4s-4 1.79-4 4a4 4 0 003.5 3.97v6.53h1V8.72A4 4 0 008 4.75z"
        try:
            meta_div = self.profile_bsoup.find("div", {"class" : "profile-metadata js-profile-metadata"})
            sub_meta_divs = meta_div.find_all("div", {"class" : "metadatum -has-label js-metadatum"})
            for sub in sub_meta_divs:
                sub.find("path", {"d" : location_pin_d})
                self.location_str = sub.find("span", {"class" : "label"}).text
                return
        except Exception as e:
            self.location_str = None
            return
        return
    #end _generate_location_str
    
    def _generate_ratings_pages(self):
        ratings_page = requests.get(self.ratings_base_url, allow_redirects=True)
        pg1_soup = BSoup(ratings_page.content, 'html.parser')
        self.ratings_pages_bsoups[self.ratings_base_url] = pg1_soup

        pagin_div = pg1_soup.find("div", {"class" : "paginate-pages"})
        if pagin_div is None:
            self.ratings_pages = 1
        else:
            list_items = pagin_div.find_all("li")
            last_page = list_items[-1]
            self.ratings_pages = int(last_page.find("a").text)
        return
    #end _generate_ratings_pages()

    def _generate_ratings_pages_bsoups(self):
        if len(self.ratings_pages_bsoups) is 0:
            page = requests.get(self.ratings_base_url, allow_redirects=True)
            soup = BSoup(page.content, 'html.parser')
            self.ratings_pages_bsoups[self.ratings_base_url] = bsoup
        #end if
        if self.time_log:
            s = (str(dt.now()) + " : pages souped (out of "+str(self.ratings_pages)+"): \n"+ d_sp+" : 1")
            print(s, end =", ")
        for i in range(1, self.ratings_pages):
            url = (self.ratings_base_url + "page/" + str(i+1) + "/")
            page = requests.get(url, allow_redirects=True)
            bsoup = BSoup(page.content, 'html.parser')
            self.ratings_pages_bsoups[url] = bsoup
            if self.time_log: print_num((i+1)) 
        #end for
        if self.time_log:
            s = ("\n" + str(dt.now()) + " : finished souping")
            print(s)
        return
    #end _generate_ratings_pages_bsoups()


    def _generate_film_info(self, film_url, film_lid, film_name):
        page = requests.get(film_url, allow_redirects=True)
        pg_soup = BSoup(page.content, 'html.parser')
        crew_div = pg_soup.find("div", {"id" : "tab-crew"})
        film_director = []
        if crew_div is not None:
            crew_link_list = crew_div.find_all("a", href=True)
            for a in crew_link_list:
                link = a['href']
                link_split = link.split("/") # -> ["", "crew-type", "name", ""]
                if link_split[1] == director:
                    film_director.append(link_split[2])
                    break
                #end if
            #end for
        #end if
        cast_div = pg_soup.find("div", {"id": "tab-cast"})
        film_actors = []
        if cast_div is not None:
            cast_link_list = cast_div.find_all("a", href=True)
            for a,i in zip(cast_link_list, range(5)): # get first 5 actors (will only run for i<5)
                link = a['href']
                link_split = link.split("/") # -> ["", "actor", "name", ""]
                film_actors.append(link_split[2])
            #end for
        #end if
        genres_div = pg_soup.find("div", {"id" : "tab-genres"})
        film_genres = []
        if genres_div is not None:
            genre_links = genres_div.find_all("a")
            for a in genre_links:
                film_genres.append(a.text)
            #end for
        #end if
        film_info_dict[film_lid] = {url : film_url,
                                    name: film_name,
                                    genre : film_genres ,
                                    director : film_director,
                                    actor : film_actors }
        return (film_genres, film_director, film_actors) # returns 3 lists: director, actors, genres
    #end _generate_film_info

    def _generate_ratings_by_page(self, bsoup):
        # add each to self.ratings dict
        cols_div = bsoup.find("section", {"class" : "section col-main overflow"})
        if cols_div is None: return

        ul_list = cols_div.find("ul", {"class" : "poster-list -p150 -grid"})
        if ul_list is None: return
        list_items = ul_list.find_all("li")
        i = 1
        for li in list_items:
            try:
                div = li.find("div")
                film_lid = div["data-film-id"]
                film_link = div["data-target-link"]
                film_name = (film_link.split("/"))[2]
                film_url = lboxd_url + film_link

                p = li.find("p")
                rating_l = p.find("span")["class"]
                rating_10_s = (rating_l[1]).split("-")[1]
                film_rating = float(rating_10_s)/2.

                new_film=" "
                if film_lid in film_info_dict:
                    film_director = film_info_dict[film_lid][director]
                    film_actors = film_info_dict[film_lid][actor]
                    film_genres = film_info_dict[film_lid][genre]
                else:
                    new_film = "n"
                    self.films_added_to_dict = self.films_added_to_dict + 1
                    (film_genres, film_director, film_actors) = self._generate_film_info(film_url, film_lid, film_name)
                #end if
                self.ratings_simple[film_lid] = film_rating
                self.ratings[film_lid] = {"rating": film_rating,
                                            url : film_url,
                                            name : film_name,
                                            genre: film_genres,
                                            director: film_director,
                                            actor: film_actors }
                if self.time_log: print_num(i, n=new_film)
                i = i+1
            except Exception as e:
                print("\nFilm Collection Failed: ", e)
                print(traceback.format_exc())
                print("content: ", li, "\n")
        #end for
        return
    #end get_rating_by_page()

    def _generate_ratings(self):
        self.ratings = {}
        self.ratings_simple = {}
        # scrapes all bsoups in self.page_bsoups for all user urls on the page
        i = 1
        for bsoup in self.ratings_pages_bsoups.values():
            if self.time_log:
                s = ("    Parsing films on pg"+str(i))
                print(s, end =": ")
            self._generate_ratings_by_page(bsoup)
            i = i+1
            if self.time_log: print()
        #end for
        return
    #end _generate_ratings()

    def _generate_average_rating(self):
        sum = 0
        count = len(self.ratings)
        for film in self.ratings:
            rating = self.ratings[film]["rating"]
            sum = sum + rating
        #end for
        self.average_rating = (sum/count)
        return
    #end _generate_average_rating

    def _generate_average_rating_by_X(self, X_str):
        ratings_by_X = {}
        # add rating for each film to dict of genres
        for film in self.ratings:
            rating = self.ratings[film]["rating"]
            Xs = self.ratings[film][X_str]
            if Xs is None: return
            for x in Xs:
                try: 
                    ratings_by_X[x].append({"film_lid" : film, "rating" : rating})
                except:
                    ratings_by_X[x]= [{"film_lid" : film, "rating" : rating}]
            #end for
        #end for
        if self.time_log:
            s = ("    Averages by "+ X_str +"("+ str(len(ratings_by_X)) +")")
            print(s, end=": ")
        # get sum/count of ratings for each x, find average value, add to average_rating_by_X
        average_rating_by_X = {}
        for x in ratings_by_X: # eg each genre found in self.ratings
            x_lids = []
            x_ratings = ratings_by_X[x]
            sum = 0.
            count = len(x_ratings)
            for film in x_ratings:
                x_lids.append(film["film_lid"])
                sum = sum + film["rating"]
            #end for
            x_avg = (sum/count)
            average_rating_by_X[x] = { "avg" : x_avg,
                                       "count" : count,
                                       "avg_minus_tot_avg" : (x_avg - self.average_rating),
                                       "film-lids" : x_lids }
        #end for
        if self.time_log: print("done")
        return average_rating_by_X
    #end _generate_average_rating_by_X()

    def print_all(self):
        print("LID = ", self.lid)
        print("    name=", self.name)
        print("    obj=", self)
        print("    profile_url=", self.profile_url)
        print("    location_str = ", self.location_str)
        print("    country = ", self.country)
        print("    #ratings pages = ", self.ratings_pages)
        print("    #ratings = ", len(self.ratings))
        print("    average rating = ", self.average_rating)
        print("    #genres = ", len(self.average_rating_by_genre))
        print("    average by genre (3) = ", self._X_str(self.average_rating_by_genre))
        print("    #directors = ", len(self.average_rating_by_director))
        print("    average by directors (3) = ", self._X_str(self.average_rating_by_director))
        print("    #actors = ", len(self.average_rating_by_actor))
        print("    average by actors (3) = ", self._X_str(self.average_rating_by_actor))
        print("    json exists = ", (self.json != None))
        print("\n\n")
        return
    #end print_all()

    def _X_str(self, avg_dict):
        sub_dict = {}
        if len(avg_dict) > 3:
            avg_keys = list(avg_dict.keys())
            for i in range(3):
                x = random.choice(avg_keys)
                sub_dict[x] = avg_dict[x]
            #end for
        else: sub_dict = avg_dict
        #end if
        x_str = ""
        for x in sub_dict:
            x_str = x_str + ("\n        " + x + "("+ str(sub_dict[x]["count"]) +") = " + str(sub_dict[x]["avg"]))
        #end for
        return x_str
    #end

    def append_films(self):
        s = "\"" + self.json["lid"] + "\""
        for lid in self.json["ratings_by_film"]:
            s = s+","+lid
        append_f(user_films_csv_path, (s+"\n"))

    def get_json(self):
        if self.json is not None:
            return self.json
        else:
            # generate json
            self.json = {
                "lid" : self.lid,
                "name" : self.name,
                "profile_url" : self.profile_url,
                "location_string" : self.location_str,
                "country" : self.country,
                "average_rating" : self.average_rating,
                "ratings_by_film" : self.ratings_simple,
                "average_rating_by_genre" : self.average_rating_by_genre,
                "average_rating_by_director" : self.average_rating_by_director,
                "average_rating_by_actor" : self.average_rating_by_actor,
            }
            self.append_films()
            return self.json
    #end get_json()

#end Letterboxd_User

def print_num(i, n=""):
    s = (str(i) + n)
    if (i % 30) is 0:
        s = s + (",\n" + d_sp + " :")
    else:
        s = s + (",")
    print(s, end=" ")
#end 
print(dt.now())

"""# helper methods

## json file minimising
makes json files smaller by removing new lines and whitespace

### 1 user
"""

rat_film = "ratings_by_film"
avg_gen = "average_rating_by_genre"
avg_dir = "average_rating_by_director"
avg_act = "average_rating_by_actor"

def field_string(field_json, field_name):
	s = json.dumps(field_json).replace("\"", "\'")
	s_to_replace = ("\'" + field_name + "\': {")
	s_replace_with = (s_to_replace + "\n")
	s = ("\'"+field_name+"\': " + s).replace(s_to_replace, s_replace_with)
	return s

def fix_json(user_json):
	new_json = copy.deepcopy(user_json) # makes copy instead of new pointer to same object
	new_json[rat_film] = []
	for g in new_json[avg_gen]:
		new_json[avg_gen][g] = []
	new_json[avg_dir] = []
	new_json[avg_act] = []
	# temporarily replace all existing " with '
	nj_s = json.dumps(new_json, indent=2)
	replace_single_quote = ""
	if nj_s.count("\'") != 0:
		if nj_s.count("<<<<>>>>") == 0: replace_single_quote = "<<<<>>>>"
		elif nj_s.count("@") == 0: replace_single_quote = "@"
		elif nj_s.count("ú") == 0: replace_single_quote = "ú"
		else:
			raise Exception("found a single quote that couldn't be replaced by any given alternate characters")
			return None
		nj_s = nj_s.replace("\'", replace_single_quote)
	#end if
	nj_s = nj_s.replace("\"", "\'")

	rf = user_json[rat_film]
	rf_s = field_string(rf, rat_film)
	nj_s = nj_s.replace("\'"+rat_film+"\': []", rf_s)

	for g_name in user_json[avg_gen]:
		g = user_json[avg_gen][g_name]
		g_s = field_string(g, g_name)
		nj_s = nj_s.replace("\'"+g_name+"\': []", g_s)

	ad = user_json[avg_dir]
	ad_s = field_string(ad, avg_dir)
	nj_s = nj_s.replace("\'"+avg_dir+"\': []", ad_s)

	aa = user_json[avg_act]
	aa_s = field_string(aa, avg_act)
	nj_s = nj_s.replace("\'"+avg_act+"\': []", aa_s)

	nj_s = nj_s.replace("\"", "") # remove all "" around lists
	nj_s = nj_s.replace("\'", "\"") # restore previous " instead of ' (invalid json)
	if replace_single_quote != "":
		nj_s = nj_s.replace(replace_single_quote, "\'")
	return nj_s

"""### users loop"""

def fix_json_mult_users(all):
	new_all_s = "{ \"users\": [\n"
	first_user = True
	i = 0
	for u in all["users"]:
		add_s = ""
		try:
			user_dump = json.dumps(u) ## user on all one line
			add_s = user_dump[0] + "\n" + user[1:] # inserts a newline after first open bracket (collapsable)
		except:
			try: s = u.lid
			except: s = "(couldn't get user.lid)"
			s = "failed on user:"+ i + ", lid=" + s +" >>\n" + traceback.format_exc()
			print(s)
			return

		if first_user:
			first_user = False
		else:
			add_s = ",\n" + add_s
		new_all_s = new_all_s + add_s
		if (i%45) == 0:
			print(i, end="\n    ")
		else: print(i, end=" ")
		i = i+1
	print()
	new_all_s = new_all_s + "\n] }"
	return new_all_s
#end

"""### 1 file"""

file_name_generic = "TYPE_users_X00_X99.json"

def fix_json_in_file(x00=0, user_type="pop"):
	file_name = (file_name_generic).replace("X", str(x00)).replace("TYPE", user_type)
	file_path = folder_path + file_name

	dest_name = (file_name).replace(".json", "_new.json")
	dest_path = (folder_path + dest_name)
 
	print(">",dest_name, "\n    users:", end=" ")
	f = open(file_path, "r")
	all = json.loads(f.read())
	f.close()
	new_all_s = fix_json_mult_users(all)
	try:
		json.loads(new_all_s)
		print("    valid json")
		f = open(dest_path, "w+")
		f.write(new_all_s)
		print("    written to file <", dest_path, ">")
		f.close()
	except Exception as e:
		try: f.close() # closes file if open, ignores if not
		except: pass
		print("\nfailed to write new file <", dest_path, "> :", e)
		print(traceback.format_exc())
		inv_path = dest_path.replace("new", "invalid")
		print("\nWriting to", inv_path, "instead")
		f = open(inv_path, "w+")
		f.write(new_all_s)
		f.close()
	return
#end

print(dt.now())

"""### fix_json_files_loop()"""

def fix_json_files_loop(range, user_type="pop"):
    for r in range:
        fix_json_in_file(r, user_type)
        print()
    return
#end

"""## misc methods

### write_film_info_dict()
"""

def write_film_info_dict(new_films = 0, printbool=True):
    if new_films > 0:
        j = json.dumps(film_info_dict)
        write_f(film_info_path, j)
        num_film = len(film_info_dict)
        if printbool: print("film_info_dict written. total films = " + str(num_film) + " [new=" +str(new_films)+"]")
    else:
        if printbool: print("no films added to film_info_dict")
    return

"""### valid_json"""

def valid_json(path):
    f = open(path, "r")
    j = f.read()
    f.close()
    try:
        json.loads(j)
        return True
    except:
        return False
#end
print(dt.now())

"""### get_csv_contents"""

def get_csv_contents(path=pop_usernames_csv_path):
    # for 1 column
    with open(path) as f:
        csv_dump = csv.reader(f)
        lid_nested_list = list(csv_dump)

    lid_list = []
    for nest in lid_nested_list:
        lid_list.append(nest[0])
    #end for
    return lid_list
#end 
print(dt.now())

"""## writing users to file

### write_user_list_to_file()
"""

def write_user_list_to_file(file_name, lid_list, time_log=True):
    path = folder_path + "writing/" + file_name
    try:
        write_f(path, "{ \"users\" : [\n")
        if time_log:
            start = dt.now()
            s = (str(start) + " : starting json file write to " + path +
                 "\n total films = " + str(len(film_info_dict)))
            print(s)
        first_entry = True
        i = 0
        for lid in lid_list:
            if time_log:
                s = ("\n\n"+d_sp+" "+str(i))
                print(s)
            user = LBoxd_User(maps_api_key, username=lid)
            if len(user.ratings) is 0: ## user has no films rated: don't write to json
                i = i+1
                continue

            user_js = (json.dumps(user.get_json())) ## no formatting
            user_js = user_js[0] + "\n" + user_js[1:]
            if first_entry:
                first_entry = False
                user_js = "\n" + user_js
            else:
                user_js = (",\n") + user_js
            append_f(path, user_js)
            
            write_film_info_dict(user.films_added_to_dict)
            i = i+1
        #end for
        append_f(path, "\n] }")

        if valid_json(path):
            s = read_f(path)
            path = path.replace("writing", "safety")
            write_f(path, s)
        else:
            s = read_f(path)
            path = path.replace("writing", "inv_json")
            write_f(path, s)


        write_film_info_dict()
        if time_log:
            end = dt.now()
            s = ("\n" + str(end) + " : finished json file write to " + path +
                 "\n" + d_sp + " : Time Taken: " + str(end-start))
            print(s)
    except Exception as e:
        append_f(path, "\n] }")
        write_film_info_dict()
        print("writing users failed :", e)
        print(traceback.format_exc())
    return
#end write_user_list_to_file()

print(dt.now())

"""### get_list_x00_x99()
get sublist of 100 users from list
"""

def get_list_x00_x99(start, lid_list):
    max = len(lid_list)
    end = start+100
    if end > max: end = max
    new_list = lid_list[start:end]
    return new_list
#end

"""### write_100_users_from_csv()"""

file_name_generic = "TYPE_users_X00_X99.json"
def write_100_users_from_csv(first_00, user_type="pop"):
    lid_list = get_csv_contents(pop_usernames_csv_path.replace("pop", user_type))
    new_list = get_list_x00_x99(first_00*100, lid_list)

    file_name = ( file_name_generic.replace("X", str(first_00)).replace("TYPE", user_type) )
    print(dt.now(), file_name)
    write_user_list_to_file(file_name, new_list)
    return
#end

"""## elliot page fixing"""

## fix names inside files
def apply_elliot_fixes():
    csv_path = folder_path + "elliot_fixes_to_do.csv"
    file_paths = read_f(csv_path).split("\n")
    for path in file_paths:
        read_in = read_f(path)
        if read_in is None: continue

        print("file:", path.split("/")[-1])
        yes = read_in.count("elliot-page")
        no = read_in.count("ellen-page")
        print("  yes=", yes)
        print("  no =", no)
        users_dict = json.loads(read_in)
        for i in range(len(users_dict["users"])):
            u = users_dict["users"][i]
            ellen_avg = {}
            elliot_avg = {}
            u_s = json.dumps(u)
            yes = u_s.count("elliot-page")
            no = u_s.count("ellen-page")
            if (no == 0): ## DN is not present
                continue
            else:
                pad = ""
                if i<10: pad = "  "
                print(" ",i, pad, "yes=", yes," no =", no, end=" ")
                ellen_avg = u["average_rating_by_actor"]["ellen-page"]
                if (yes == 0): ## DN is present, elliot is not
                    u["average_rating_by_actor"]["elliot-page"] = ellen_avg # creates elliot
                    u["average_rating_by_actor"].pop("ellen-page") # deletes ellen
                    print("    replacing")
                else: ## both names are present, *merge*
                    print("    merging")
                    elliot_avg = u["average_rating_by_actor"]["elliot-page"]
                    tmplist = elliot_avg["film-lids"]
                    for lid in ellen_avg["film-lids"]:
                        tmplist.append(lid)
                    # all films added to lids
                    sum = 0
                    count = len(tmplist)
                    for lid in tmplist:
                        sum = sum + u["ratings_by_film"][lid]
                    # sum of all new_page film ratings
                    avg = sum/count
                    new_page = {
                        "avg" : (avg),
                        "count" : count,
                        "avg_minus_tot_avg" : (avg-u["average_rating"]),
                        "film-lids" : tmplist
                    }
                    # filled new dict
                    u["average_rating_by_actor"]["elliot-page"] = new_page # replaces 'elliot-page'
                    u["average_rating_by_actor"].pop("ellen-page") # deletes ellen
                #end if yes
            #end if no
            users_dict["users"][i] = u # updates user in dict
        #end for users
        write_elliot(path.replace("safety", "elliot"), users_dict)
    #end for paths
    print("DONE")
    return
#end

def write_elliot(path, out_dict):
    write_f(path, "{ \"users\" : [\n")
    first = True
    for u in out_dict["users"]:
        user_js = (json.dumps(u)) ## no formatting
        user_js = user_js[0] + "\n" + user_js[1:] ## a little formatting, as a treat
        if first: first = False
        else: user_js = (",\n") + user_js
        append_f(path, user_js)
    #end for
    append_f(path, "\n] }")
    print("  written")

    check = read_f(path)
    yes = check.count("elliot-page")
    no = check.count("ellen-page")
    print("  new yes=", yes)
    print("  new no =", no)
    return

#apply_elliot_fixes()

elliot_fixes = folder_path + "elliot_fixes_to_do.csv"
## replace all instances of "ellen-page" with "elliot-page" in ALL files where only DN occurs
## writes others to csv

def film_dict_replace_elliot_page():
    fi_path = film_info_path.replace("data", "data/elliot").replace(".json", "_old.json")
    fi_old = read_f(film_info_path)
    fi_new = fi_old.replace("ellen-page", "elliot-page")
    print("film info string replaced")
    write_f(fi_path, fi_old)
    write_f(film_info_path, fi_new)
    print("film info written\n")
    return;

def file_replace_elliot_page(ulist, user_type="pop"):
    uj_path = (folder_path + "safety/TYPE_users_X00_X99.json").replace("TYPE", user_type)
    u_start = dt.now()
    print(user_type, ":", u_start)
    for i in ulist:
        i_path = uj_path.replace("X", str(i))
        i_s = read_f(i_path)
        if i_s is None: continue

        pad = ""
        if i<10: pad = "  "
        print(i_path.split("/")[-1], pad, ". read", end=" . ")
        yes = i_s.count("elliot-page")
        no = i_s.count("ellen-page")
        print("yes=", yes, ". no =", no, end=" . ")
        if (no) == 0:
            # if file contains neither, no action needed
            # if file contains 'elliot-page' but not 'ellen-page', no action needed
            print("skip")
            continue
        else:
            if (yes) == 0:
                # if file contains 'ellen-page' but not 'elliot-page', straight replace
                i_s = i_s.replace("ellen-page", "elliot-page")
                print("replaced", end=" . ")
                write_f(i_path, i_s)
                print("written")
                u_prev = dt.now()
            else:
                # if file contains both: add to csv to deal with later
                s = i_path + "\n"
                append_f(elliot_fixes, s)
                print("added to csv")
                u_prev = dt.now()
            #end if
        #end if
    #end for
    print("all", user_type, "parsed in", (dt.now()-u_start))
    return
#end

gen_file = "safety/TYPE_users_X00_X99.json"
def elliot_checkfix_all_files():
    write_f(elliot_fixes, "")
    print("clearing csv\n")
    
    ns = []
    for n in range(39):
        ns.append(n)
    file_replace_elliot_page(ns, user_type="pop")
    print()

    for n in range(39,58):
        ns.append(n)
    file_replace_elliot_page(ns, user_type="gen")
    print()
    
    apply_elliot_fixes()
    return
#end

"""# Running

### check all files in writing/ exist and are valid
"""

check_path = folder_path + "writing/TYPE_users_X00_X99.json"
def check_all_exists():
    # 0 - 57 gen
    # 0 - 38 pop
    ppath = check_path.replace("TYPE", "pop")
    for p in range(39):
        s = read_f(ppath.replace("X", str(p)))
        if s is None:
            print("can't find pop users", p)
        else:
            try:
                json.loads(s)
            except JSONDecodeError as jde:
                print("got error for pop", p, ":", jde)
    #end for
    print("read all pop")

    upath = check_path.replace("TYPE", "gen")
    for u in range(58):
        s = read_f(upath.replace("X", str(u)))
        if s is None:
            print("can't find gen users", u)
        else:
            try:
                json.loads(s)
            except JSONDecodeError as jde:
                print("got error for gen", u, ":", jde)
    #end for
    print("read all gen")
    return
#end

check_all_exists()

"""### write_users_range()"""

def write_users_range(list_range=None, max=0, min=0, user_type="pop"):
    if list_range is None:
        for i in range(min, max+1):
            write_100_users_from_csv(i, user_type=user_type)
        return
    for i in list_range:
        write_100_users_from_csv(i, user_type=user_type)
    return
#end

#write_users_range(list_range=[56,57], user_type="gen")
#write_users_range(max=55, min=1, user_type="gen")
write_users_range(max=31, min=30, user_type="pop")


## 54 not valid: "{ [ {"lid"..." -> "{ "users" : [ {lid..."

"""### fix invalid gen jsons"""

src_path = folder_path + "gen_users_X00_X99.json"

for i in range(0):
    i_path = src_path.replace("X", str(i))
    i_str = "{ \"users\" : [\n" + read_f(i_path)
    try:
        i_json = json.loads(new_str_i) # checks if new json is valid
        print("    new json valid")
        write_f(i_path, new_str_i)
        print("    written", i_path)
    except JSONDecodeError as jde:
        print("    json invalid", jde)
        inv_path = i_path.replace("99", "99_invalid").replace("gen", "inv_json/gen")
        write_f(inv_path, new_str_i)
        print("    written to", inv_path)

"""### apply_elliot_fixes()
goes through files where both names exist and handles each user in turn
"""

def apply_elliot_fixes():
    csv_path = folder_path + "elliot_fixes.csv"
    file_paths = read_f(csv_path).split("\n")
    for path in file_paths:
        print("file: ", path.split("/")[-1] )
        users_dict = json.loads(read_f(path))
        for i in range(len(users_dict["users"])):
            print(" ",i, end=": ")
            u = users_dict["users"][i]
            ellen_avg = {}
            elliot_avg = {}
            try:
                ellen_avg = u["average_rating_by_actor"]["ellen-page"]
            except KeyError:
                # user does not have ellen-page as an actor
                print("skipped")
                continue # to next user
            try:
                elliot_avg = u["average_rating_by_actor"]["elliot-page"]
            except KeyError:
                # user does not have elliot-page but *does* have ellen-page
                users_dict["users"][i]["average_rating_by_actor"]["elliot-page"] = ellen_avg
                print("straight replace")
                continue # to next user
            # user has both elliot-page and ellen-page
            # todo: merge
            new_page = {}
            new_page["film-lids"] = elliot_avg["film-lids"]
            for lid in ellen_avg["film-lids"]:
                new_page["film-lids"].append(lid)
            new_page_ratings = {}
            for lid in new_page["film-lids"]:
                new_page_ratings[lid] = u["ratings_by_film"][lid]

"""# other running

### general_users: remove_duplicates()
"""

def rm_duplicates():
    philip_path = folder_path.replace("Sarah_data", "Philip's Data")
    s = read_f((philip_path+"all_users.json"))
    users = json.loads(s)
    print("on open:", len(users))
    pop_users = get_csv_contents(pop_usernames_csv_path)
    for u in pop_users:
        try:
            users.remove(u)
        except ValueError:
            pass
    print("rm populars:", len(users))
    ## remove internal duplicates
    users_2 = (list(dict.fromkeys(users)))
    print("rm duplicates:", len(users_2))
    write_user_lids(users_2, (gen_usernames_csv_path))
    print("written")
    return
#end

"""### validate_json()
loads in jsons and checks if valid. prints #users if valid
"""

def validate_json(list_range=None, max_00=38, new=True, user_type="pop"):
    path = (folder_path + "TYPE_users_X00_X99.json").replace("TYPE", user_type)
    if new: path = path.replace(".json", "_new.json")
    if list_range is None:
        list_range = []
        for j in range(max_00+1): list_range.append(j)
        
    for i in list_range:
        i_path = path.replace("X", str(i))
        s = (str(i) + " - <" + i_path + ">" + ":")
        print(s)
        try:
            f = open(i_path, "r")
            i_json = json.loads(f.read())
            print("    json valid", end=": ")
            num = len(i_json["users"])
            print(" len('users') =", num)
        except FileNotFoundError as fnfe:
            print("    couldn't find", path)
        except JSONDecodeError as jde:
            print("    json invalid", jde)
        except Exception as e:
            print("    oops", e)
        #end try
        print()
    #end for


print(dt.now())
#validate_json(max_00=4, new=False, user_type="gen")

"""### file_replace_elliot_page()
checks for files where only 'ellen-page' exists and replaces with 'elliot-page'
adds files where both exists to csv 'elliot_fixes' to be dealt with
"""

elliot_fixes = folder_path + "elliot_fixes_to_do.csv"
## replace all instances of "ellen-page" with "elliot-page" in ALL files where only DN occurs
## writes others to 

def film_dict_replace_elliot_page():
    fi_path = film_info_path.replace(".json", "_old.json")
    fi_old = read_f(film_info_path)
    fi_new = fi_old.replace("ellen-page", "elliot-page")
    print("film info string replaced")
    write_f(fi_path, fi_old)
    write_f(film_info_path, fi_new)
    print("film info written\n")
    return;

def file_replace_elliot_page(ulist, user_type="pop"):
    uj_path = (folder_path + "safety/TYPE_users_X00_X99.json").replace("TYPE", user_type)
    u_start = dt.now()
    print(user_type, ":", u_start)
    for i in ulist:
        i_path = uj_path.replace("X", str(i))
        print(i_path)
        i_s = read_f(i_path)
        print("    ", i, "file read")
        if (i_s.find("ellen-page")) == -1:
            # if file contains neither, no action needed
            # if file contains 'elliot-page' but not 'ellen-page', no action needed
            print("    no replacement needed")
            continue
        else:
            if (i_s.find("elliot-page")) == -1:
                # if file contains 'ellen-page' but not 'elliot-page', straight replace
                i_s = i_s.replace("ellen-page", "elliot-page")
                print("    straight replace done")
                write_f(i_path, i_s)
                print("    written")
                u_prev = dt.now()
            else:
                # if file contains both: add to csv to deal with later
                
                s = i_path + "\n"
                append_f(elliot_fixes, s)
                print("    contains both: added to elliot_fixes_to_do.csv ")
                u_prev = dt.now()
            #end if
        #end if
    #end for
    print("\nall user json files parsed in ", (dt.now()-u_start))
    return
#end

ulist_gen = []
for i in range(5,56):
    ulist_gen.append(i)
print(dt.now())
#file_replace_elliot_page(ulist_gen, user_type="gen")

#film_dict_replace_elliot_page()

"""### rename_json()

"""

src_file = "pop_users/pop_users_X00_X99_new.json"
dst_file = "pop_users_X00_X99.json"

def rename_json():
    for i in range(39):
        print(i, ":")
        dst_path = folder_path + dst_file.replace("X", str(i))
        src_path = folder_path + src_file.replace("X", str(i))
        i_s = read_f(old_path)
        try:
            i_json = json.loads(i_s)
            print("    json loaded", end=" ")
            write_f(dst_path, i_s)
            s = ("(written to " + dst_path + ")")
            print(s)
        except Exception as e:
            print(" noped out because", e, "\n", traceback.format_exc(), "\n")
            continue
        #end try
    #end for
#end

"""### other"""

#write_f(user_films_csv_path, "")
print(user_films_csv_path)

"""
s = read_f(folder_path+"gen_users_5400_5499.json")
s = s[0] + "\"users\" :" + s[1:]
json.loads(s)
write_f(folder_path+"safety/gen_users_5400_5499.json", s)
"""

fp = folder_path+"safety/gen_users_X00_X99.json"
def append_user_films()
    for i in range(5,54):
        print(i, end=" : ")
        fi = fp.replace("X", str(i))
        all = json.loads(read_f(fi))
        print("loaded", end=", ")
        for u in all["users"] :
            s = "\"" + u["lid"] + "\""
            for lid in u["ratings_by_film"]:
                s = s+","+lid
            append_f(user_films_csv_path, (s+"\n"))
        print("all users appended")
#end

"""# OTHER

## Possible Modelling Methods + Tests
"""

folder_path = "/content/drive/MyDrive/4th_Year/ML Group Project/Sarah_data/safety/"

## uses old unmerged film data from Philip's Data/all_filmsV2.json 10/Dec/2020 8pm

"""
{  "34722": {
    "name": "Inception",
    "url": "/film/inception/",
    "lid": "34722",
    "tmdb_id": "27205",
    "number_of_ratings": 536165,
    "avg_rating": 4.18,
    "genres": [
      "Science Fiction",
      "Action",
      "Adventure"
    ],
    "director_url": "/director/christopher-nolan/",
    "actors_urls": [
      "/actor/leonardo-dicaprio/",
      "/actor/ken-watanabe/",
      "/actor/joseph-gordon-levitt/",
      "/actor/marion-cotillard/",
      "/actor/elliot-page/"
    ],
    "number_of_likes": 353380,
    "number_of_views": 926820
  }}
"""

def get_relevant_fields(user_json, film_json):
    relevant = {}
    relevant["user_lid"] = user_json["lid"]
    relevant["film_lid"] = film_json["lid"]
    relevant["user_country"] = user_json["country"]
    relevant["user_films_watched"] = len(user_json["ratings_by_film"])
    relevant["film_total_watches"] = film_json["number_of_views"]
    relevant["film_total_likes"] = film_json["number_of_likes"]
    relevant["director"] = try_json(user_json, film_json, "director")
    for i in range(len(film_json["genres"])):
        key = film_json["genres"][i]
        relevant[key] = try_json(user_json, film_json, "genre", index=i)
    for i in range(5):
        key = "actor"+str(i)
        relevant[key] = try_json(user_json, film_json, "actor", index=i)
    print(json.dumps(relevant, indent=2))
    return relevant
#end

def try_json(user_json, film_json, field, index=0):
    if field == "director":
        x = film_json[(field+"_url")]
        x = x.split("/")[2]
    elif field == "actor":
        x = film_json[(field+"s_urls")][index]
        x = x.split("/")[2]
    elif field == "genre":
        #print(film_json)
        x = film_json[(field+"s")][index]
    else: return None
    print(x, end=" : ")
    #end if
    try:
        u_x_avg = user_json[("average_rating_by_"+field)][x]["avg_minus_tot_avg"]
        print("got x")
        return u_x_avg
    except KeyError as ke:
        print("no x", ke)
        return None
    except Exception as e:
        print("Something happened: ", e)
        print(traceback.format_exc())
    #end try
#end

def test_get_revelant_fields():
    inception_lid = "34722"
    film_data_path = folder_path.replace("Sarah_data/safety/", "Philip's Data/all_filmsV2.json")
    all_films = json.loads(read_f(film_data_path).lower())
    inception = all_films[inception_lid]
    #print(json.dumps(inception,indent=2), "\n")

    # user who has watched Inception: "captstevezissou" from 000_099.json
    captstevezissou = {}
    all = json.loads( read_f((folder_path+"gen_users_5100_5199.json")) )
    for u in all["users"]:
        if u["lid"] == "captstevezissou":
            captstevezissou = u
            break
    #end for
    #print(json.dumps(captstevezissou, indent=2), "\n")
    get_relevant_fields(captstevezissou, inception)
    return

print(dt.now())
#test_get_revelant_fields()

"""## LBoxd_populars
Scraping usernames from letterboxd most popular reviewers pages
"""

def CSV_write_all_popular_usernames():
    pop = LBoxd_populars()
    pop.write_user_lids(pop_usernames_csv_path)
#end
#CSV_write_all_popular_usernames()

populars_url = "https://letterboxd.com/reviewers/popular/this/all-time/"
max_page = 128


class LBoxd_populars():
    def __init__(self, url=populars_url, pages=max_page, time_log=True):
        self.time_log = time_log
        if self.time_log:
            setup_time_start = dt.now()
            print(setup_time_start, " : START popular")

        self.base_url = url
        if isinstance(pages, int) and pages >= 1:
            self.pages = pages
        else:
            raise TypeError("pages must be an integer >= 1")

        self.page_bsoups = {} # dict {url : bsoup_obj}
        self._fill_page_bsoups()
        self._generate_users_lid_list()

        if self.time_log: 
            end_time = dt.now()
            print(end_time, " : ", str(end_time - setup_time_start), " : END\n")
    #end __init__

    def _fill_page_bsoups(self):
        if self.time_log:
            s = (str(dt.now())+" : pages (max:"+self.pages+")")
            print(s, end=": ")
        for i in range(self.pages):
            pg = i+1
            if pg is 1:
                url = self.base_url
            else:
                url = (self.base_url + "page/" + str(pg) + "/")
            page = requests.get(url, allow_redirects=True)
            bsoup = BSoup(page.content, 'html.parser')
            self.page_bsoups[url] = bsoup
            if self.time_log: print(i, end=", ")
        #end for
        if self.time_log:
            s = ("\n", dt.now(), " : finished souping")
            print(s)
        return
    #end fill_page_bsoups

    def _generate_users_lid_list(self):
        # scrapes all bsoups in self.page_bsoups for all user urls on each page
        lids = []
        pg = 1
        i = 1
        for bsoup in self.page_bsoups.values():
            if self.time_log:
                s = (str(dt.now())+" : parsing users on page "+str(pg))
                print(s, end=": ")
            pg = pg+1
            table = bsoup.find("table", {"class" : "person-table"})
            table_body = table.find("tbody")
            rows = table_body.find_all("tr")
            
            for row in rows:
                h3 = row.find("h3", {"class" : "title-3"})
                user_link = h3.find('a', href=True)['href'] # '/lid/'
                user_lid = user_link.split("/")[1]
                lids.append(user_lid)
                if self.time_log: print(i, end=", ")
                i = i+1
            #end for
            print()
        #end for
        self.users_lid_list = lids
        return
    #end _generate_users_lid_list()

    def write_user_lids(self, path):
        f = open(path, "w+")
        file_s = ""
        for lid in self.users_lid_list:
            file_s = file_s + (lid + "\n")
        #end for
        f.write(file_s)
        return
    #end

    def print_all(self, random_users=0):
        print(self)
        print("url: ", self.base_url)
        print("pages: ", self.pages)
        print("#page_bsoups: ", len(self.page_bsoups))
        print("#LBoxd Users: ", len(self.users_lid_list))
        
        if random_users > 0:
            print("\nrandom user lids: ")
        for i in range(random_users):
            print("    ",random.choice(self.users_lid_list))
        #end for
        return
    #end print_all()

#end LBoxd_populars

print(dt.now())